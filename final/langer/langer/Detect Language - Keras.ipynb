{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Language from Text\n",
    "\n",
    "In this project we will focus on Text Classification. Text Classification has been used in the past for [Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis). Here, we will use text classification to predict the language of the input text. \n",
    "\n",
    "In order to classify text, we will look at a class of Neural Network where connections between units form a directed cycle, called Recurrent Neural Network (RNNs). RNNs use an internal memory to process sequences of elements and is able to learn from the syntactic structure of text. Our model will be able to classify text based on the text we train it with.\n",
    "\n",
    "RNNs are [very effective](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) when understanding sequence of elements and have been used in the past to generate or classify text. I will use a Recurrent Neural Network to predict the language of the input text.\n",
    "\n",
    "The solution below, can be useful in the following scenarios:\n",
    "- A Global Customer Service team receives all their requests via a single email address. The model below can be used to identify the language of the text and forward it to the right person/group.\n",
    "- Tagging an email.\n",
    "- A Government agency of a country that has several regional languages, e.g. Spain, which is what we've used in this notebook to train our model.\n",
    "\n",
    "Alternative solutions:\n",
    "\n",
    "The Translation API by Google can be used to detect the language of a text.\n",
    "https://cloud.google.com/translate/docs/detecting-language\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "The purpose of this project, is to create a model that will be able to identify the language the input text is in.\n",
    "\n",
    "The performance of our model will be measure by the accuracy of identifying the right language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Input\n",
    "\n",
    "To train our model we will use the [recognised languages](https://en.wikipedia.org/wiki/Spain#cite_note-c-6) in [Spain](https://en.wikipedia.org/wiki/Spain) as input. Apart from the official language - Castilian (Spanish), there are different languages spoken in various autonomous communities, e.g. Catalan, Gailicia, Basque, Aragonese, Asturian and Occitan.\n",
    "\n",
    "The text used as input comes from Wikipedia. We are using the Leipzig Corpora Collection - http://wortschatz.uni-leipzig.de/en/download/ dataset available for each language. We've trained the model with ~ 10,000 sentences per language. \n",
    "\n",
    "Each dataset, e.g. [Asturian](http://pcai056.informatik.uni-leipzig.de/downloads/corpora/ast_wikipedia_2016_10K.tar.gz), contains a file with sentences, minimal preprocessing (remove index and append the language name) was done to this file to train our model.\n",
    "\n",
    "The file included in the Github repository (all_sentences.txt) doesn't require further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69347</td>\n",
       "      <td>69347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>69347</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Descrición Miden de 35 a 41 cm de longo e pesa...</td>\n",
       "      <td>Galego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence language\n",
       "count                                               69347    69347\n",
       "unique                                              69347        7\n",
       "top     Descrición Miden de 35 a 41 cm de longo e pesa...   Galego\n",
       "freq                                                    1    10000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets extract our sentences and have a look at the data we will be dealing with\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"all-sentences.txt\", names=[\"sentence\", \"language\"], header=None, delimiter=\"|\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    '''Removes all special characters from sentence. It will also strip out\n",
    "    extra whitespace and makes the string lowercase.\n",
    "    '''\n",
    "    return re.sub(r'[\\\\\\\\/:*«`\\'?¿\";!<>,.|]', '', sentence.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "# As our sentences in all_sentences.txt are in order, we need to shuffle it first.\n",
    "sss = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "\n",
    "# Clean the sentences\n",
    "X = data[\"sentence\"].apply(process_sentence)\n",
    "y = data[\"language\"]\n",
    "\n",
    "# Split all our sentences\n",
    "elements = (' '.join([sentence for sentence in X])).split()\n",
    "\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55477, 13870)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages in our dataset: {'Català', 'Castellano', 'Galego', 'Asturianu', 'Euskara', 'Aragonés', 'Occitan'}\n"
     ]
    }
   ],
   "source": [
    "languages = set(y)\n",
    "print(\"Languages in our dataset: {}\".format(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "\tTrain set: \t\t(55477,) \n",
      "\tTest set: \t\t(13870,)\n",
      "Totals:\n",
      "\tWords in our Dataset: 1348815\n",
      "\tLanguages: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Shapes:\")\n",
    "print(\"\\tTrain set: \\t\\t{}\".format(X_train.shape),\n",
    "      \"\\n\\tTest set: \\t\\t{}\".format(X_test.shape))\n",
    "print(\"Totals:\\n\\tWords in our Dataset: {}\\n\\tLanguages: {}\".format(len(elements), len(languages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19143    de ben segur tota aquesta experiència pot resu...\n",
       " 29754    aital de 1847 a 1855 a lu c lo bastiment pels ...\n",
       " 12954                              aquesta actuació que ha\n",
       " 69009    va estar o president de chuntos por laragonés ...\n",
       " 40497    característiques el bacaláu en salazón emplegá...\n",
       " 19654    a auga provén do encoro de cecebre e potabilíz...\n",
       " 57217    napoleondar gerrak iritsi arte suediarren esku...\n",
       " 356      ck mire las personas que están trabajando en e...\n",
       " 45360    la so función nun ye bien conocida pero piénsa...\n",
       " 24075    estrutura os baldaquinos pétreos galegos eran ...\n",
       " Name: sentence, dtype: object, 19143        Català\n",
       " 29754       Occitan\n",
       " 12954        Català\n",
       " 69009      Aragonés\n",
       " 40497     Asturianu\n",
       " 19654        Galego\n",
       " 57217       Euskara\n",
       " 356      Castellano\n",
       " 45360     Asturianu\n",
       " 24075        Galego\n",
       " Name: language, dtype: object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at our training data\n",
    "X_train[:10], y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"Create lookup tables for vocabulary\n",
    "    :param text: The text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab = set(text)\n",
    "    \n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {v:k for k, v in vocab_to_int.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of our dataset: 187734\n"
     ]
    }
   ],
   "source": [
    "elements.append(\"<UNK>\")\n",
    "\n",
    "# Map our vocabulary to int\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(elements)\n",
    "languages_to_int, int_to_languages = create_lookup_tables(y)\n",
    "\n",
    "print(\"Vocabulary of our dataset: {}\".format(len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_int(data, data_int):\n",
    "    \"\"\"Converts all our text to integers\n",
    "    :param data: The text to be converted\n",
    "    :return: All sentences in ints\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    for sentence in data: \n",
    "        all_items.append([data_int[word] if word in data_int else data_int[\"<UNK>\"] for word in sentence.split()])\n",
    "    \n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert our inputs\n",
    "X_test_encoded = convert_to_int(X_test, vocab_to_int)\n",
    "X_train_encoded = convert_to_int(X_train, vocab_to_int)\n",
    "\n",
    "y_data = convert_to_int(y_test, languages_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "enc.fit(y_data)\n",
    "\n",
    "# One hot encoding our outputs\n",
    "y_train_encoded = enc.fit_transform(convert_to_int(y_train, languages_to_int)).toarray()\n",
    "y_test_encoded = enc.fit_transform(convert_to_int(y_test, languages_to_int)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]] \n",
      " 19143        Català\n",
      "29754       Occitan\n",
      "12954        Català\n",
      "69009      Aragonés\n",
      "40497     Asturianu\n",
      "19654        Galego\n",
      "57217       Euskara\n",
      "356      Castellano\n",
      "45360     Asturianu\n",
      "24075        Galego\n",
      "Name: language, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Sample of our encoding\n",
    "print(y_train_encoded[:10],'\\n', y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_sentence_length = 200\n",
    "embedding_vector_length = 300\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          56320200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200, 256)          570368    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 57,417,679\n",
      "Trainable params: 57,417,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    \n",
    "    # Truncate and pad input sentences\n",
    "    X_train_pad = sequence.pad_sequences(X_train_encoded, maxlen=max_sentence_length)\n",
    "    X_test_pad = sequence.pad_sequences(X_test_encoded, maxlen=max_sentence_length)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(len(vocab_to_int), embedding_vector_length, input_length=max_sentence_length))\n",
    "    model.add(LSTM(256, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))\n",
    "    model.add(LSTM(256, dropout=dropout, recurrent_dropout=dropout))\n",
    "    model.add(Dense(len(languages), activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "55477/55477 [==============================] - 73s - loss: 0.1056 - acc: 0.9607    \n",
      "Epoch 2/2\n",
      "55477/55477 [==============================] - 71s - loss: 0.0104 - acc: 0.9966    \n",
      "Accuracy: 99.53%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train_pad, y_train_encoded, epochs=2, batch_size=256)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_pad, y_test_encoded, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "After 2 epochs, our model is able to achieve an accuracy of 99.57%. Which is quite good compared to the time it will take someone to classify the text and that the person will need to be familiar with all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    \"\"\"Converts the text and sends it to the model for classification\n",
    "    :param sentence: The text to predict\n",
    "    :return: string - The language of the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean the sentence\n",
    "    sentence = process_sentence(sentence)\n",
    "    \n",
    "    # Transform and pad it before using the model to predict\n",
    "    x = numpy.array(convert_to_int([sentence], vocab_to_int))\n",
    "    x = sequence.pad_sequences(x, maxlen=max_sentence_length)\n",
    "    \n",
    "    prediction = model.predict(x)\n",
    "    \n",
    "    # Get the highest prediction\n",
    "    lang_index = numpy.argmax(prediction)\n",
    "    \n",
    "    return int_to_languages[lang_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asturianu'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"foi Vicepresidente de la institución y responsable del programa de formación de la mesma, dirixendo la UABRA. Al empar, foi l'entamador y direutor de la coleición académica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Català'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Els socis a l’Ajuntament de Barcelona han pactat la discrepància (de fet, divendres van votar en sentit contrari al ple municipal en una moció presentada per Ciutadans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asturianu'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Espardióse hacia'l sur cola Reconquista y cola conversión de Castiella nel reinu con más puxu de los cristianos, impúnxose como llingua de la población na mayor parte de lo que dempués sedría España.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Euskara'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Eklipse bat gorputz bat beste baten itzalean sartzen denean gertatzen den fenomeno astronomikoa da. Alegia, argizagi baten estaltzea, haren eta begiaren artean tartekatzen den beste argizagi batek eragina.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Galego'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Como intelectual os seus amplos desenvolvementos teóricos e filosóficos do marxismo produciron o marxismo-leninismo, unha aplicación pragmática rusa do marxismo que fai fincapé no papel fundamental\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Occitan'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"L’èuro es la moneda comuna dels 28 estats de l’Union Europèa (UE) — e la moneda unica de 19 estats membres pel moment — que succedís a l’ECU (European Currency Unit, unitat de compte europèa) que n'èra la moneda comuna.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Castellano'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Así se tiene, por ejemplo, que tanto el trigo como los otros cereales se han empleado en Europa y parte de África; el maíz es frecuente en América; el arroz, en Asia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aragonés'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"America d'o Sur, tamién clamada Sudamerica, ye un subcontinent trescruzato por l'Equador, con a mayor parti d'a suya aria en l'hemisferio sud. Ye situato entre l'Ocián Pacifico y l'Ocián Atlantico.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aragonés'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"O vin ye una bebida alcoholica obtenita d'a fermentación alcoholica, por l'acción d'os recientos, d'o suco u mosto d'os fruitos d'a planta Vitis vinifera (vinyers) que transforma os zucres d'o fruito en alcohol etilico y gas anhidrido carbonico.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Galego'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"As Xornadas de Xullo de 1917 foi o nome que recibiron as protestas armadas apoiadas polos anarcocomunistas e os bolxeviques, finalmente fracasadas, que trataron de derrocar ao Goberno Provisional Ruso e traspasar o poder aos soviets (consellos) en xullo dese ano.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Català'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"El monestir té 2.000 metres quadrats d'extensió i és de planta irregular. Al llarg dels més de 500 anys de la seva història, ha passat per diverses modificacions, sobretot arran del terratrèmol de Lisboa de 1755.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Galego'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Esta plusvalía é apropiada polo capitalista e dela procede a ganancia. Esta apropiación constitúe a base fundamental do modo de produción capitalista e a súa vez estas condicións materiais determinan a superestrutura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Castellano'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Putin cambia a su embajador en Washington, figura clave de la trama rusa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Català'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Erigit durant els segles XIV-XV, hi destaquen pel seu interès artístic l'església d'estil gòticmudèjar, així com les estances decorades amb frescos de Daniel Vázquez Díaz, el claustre i el museu, on es conserven nombrosos objectes commemoratius del descobriment d'Amèrica, i una escultura de l'advocació mariana sota la qual es troba el convent,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
